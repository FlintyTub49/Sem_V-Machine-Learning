{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiment 10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPHeAO5guolrxgfl8ZIXpZK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FlintyTub49/Sem_V-Machine-Learning/blob/master/Exercise%2010/Exercise%2010.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyYDgaMPYYw5"
      },
      "source": [
        "## Importing The Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdecU9cJYPDe",
        "outputId": "3990c416-0167-40cc-b441-7df6df2f3242",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from math import sqrt\n",
        "\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import scipy as sp\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import RegressorChain\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn import linear_model\n",
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPJEQ__DZBRC"
      },
      "source": [
        "## Donwloading and Reading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6YXkQhiZEuL",
        "outputId": "392d02da-a369-482b-e400-de47216b37a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "!wget -O train.csv.zip https://github.com/pranavn91/blockchain/blob/master/train.csv.zip?raw=true\n",
        "!wget -O test.csv.zip https://github.com/pranavn91/blockchain/blob/master/test.csv.zip?raw=true\n",
        "!unzip train.csv.zip\n",
        "!unzip test.csv.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-20 05:17:07--  https://github.com/pranavn91/blockchain/blob/master/train.csv.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/pranavn91/blockchain/raw/master/train.csv.zip [following]\n",
            "--2020-10-20 05:17:07--  https://github.com/pranavn91/blockchain/raw/master/train.csv.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/pranavn91/blockchain/master/train.csv.zip [following]\n",
            "--2020-10-20 05:17:07--  https://raw.githubusercontent.com/pranavn91/blockchain/master/train.csv.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5237160 (5.0M) [application/zip]\n",
            "Saving to: ‘train.csv.zip’\n",
            "\n",
            "train.csv.zip       100%[===================>]   4.99M  12.5MB/s    in 0.4s    \n",
            "\n",
            "2020-10-20 05:17:08 (12.5 MB/s) - ‘train.csv.zip’ saved [5237160/5237160]\n",
            "\n",
            "--2020-10-20 05:17:08--  https://github.com/pranavn91/blockchain/blob/master/test.csv.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/pranavn91/blockchain/raw/master/test.csv.zip [following]\n",
            "--2020-10-20 05:17:11--  https://github.com/pranavn91/blockchain/raw/master/test.csv.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/pranavn91/blockchain/master/test.csv.zip [following]\n",
            "--2020-10-20 05:17:11--  https://raw.githubusercontent.com/pranavn91/blockchain/master/test.csv.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2203010 (2.1M) [application/zip]\n",
            "Saving to: ‘test.csv.zip’\n",
            "\n",
            "test.csv.zip        100%[===================>]   2.10M  8.57MB/s    in 0.2s    \n",
            "\n",
            "2020-10-20 05:17:12 (8.57 MB/s) - ‘test.csv.zip’ saved [2203010/2203010]\n",
            "\n",
            "Archive:  train.csv.zip\n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: train.csv               \n",
            "Archive:  test.csv.zip\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: test.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujVE885TZF8f",
        "outputId": "6cd7d195-cf78-4e2e-a006-3f7251a864d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train = pd.read_csv(\"train.csv\", error_bad_lines=False)\n",
        "test = pd.read_csv(\"test.csv\", error_bad_lines=False)\n",
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(77946, 28)\n",
            "(42157, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3icS_vkZY_9"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0iY-lodZtZ5"
      },
      "source": [
        "### Keeping Relevant Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Gmls7tdZO-g"
      },
      "source": [
        "target_cols = train.columns.difference(test.columns)\n",
        "target = train[target_cols]\n",
        "train = train.drop(target_cols, axis = 1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JfNdVgZw5y"
      },
      "source": [
        "### Cleaning Location and State Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNXz3P4gZifX"
      },
      "source": [
        "for data in (train,test):\n",
        "    missing_loc_idx = data.index[data.location.isnull()].tolist()\n",
        "    missing_loc_dict = {}\n",
        "    for i in range(len(data)):\n",
        "        if i in missing_loc_idx:\n",
        "            missing_loc_dict[i] = 1\n",
        "        else:\n",
        "            missing_loc_dict[i] = 0\n",
        "    data['missing_loc'] = data.index.map(missing_loc_dict)\n",
        "\n",
        "for data in (train,test):\n",
        "    missing_state_idx = data.index[data.state.isnull()].tolist()\n",
        "    missing_state_dict = {}\n",
        "    for i in range(len(data)):\n",
        "        if i in missing_state_idx:\n",
        "            missing_state_dict[i] = 1\n",
        "        else:\n",
        "            missing_state_dict[i] = 0\n",
        "    data['missing_state']= data.index.map(missing_state_dict)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIf_8MsmaA9g"
      },
      "source": [
        "### Replacing NA Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzFXjhAdZ1LS"
      },
      "source": [
        "for data in (train,test):\n",
        "    data['location'] = data['location'].replace(np.nan, '', regex=True)\n",
        "    data['state'] = data['state'].replace(np.nan, '', regex=True)\n",
        "    data['full_text'] = data['tweet']+' '+data['state']+' '+data['location']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQQga0exaDcp"
      },
      "source": [
        "states = {\"AL\":\"Alabama\", \"AK\":\"Alaska\", \"AZ\":\"Arizona\", \"AR\":\"Arkansas\", \"CA\":\"California\", \"CO\":\"Colorado\", \"CT\":\"Connecticut\", \n",
        "          \"DE\":\"Delaware\", \"FL\":\"Florida\", \"GA\":\"Georgia\", \"HI\":\"Hawaii\", \"ID\":\"Idaho\", \"IL\":\"Illinois\", \"IN\":\"Indiana\", \"IA\":\"Iowa\",\n",
        "          \"KS\":\"Kansas\", \"KY\":\"Kentucky\", \"LA\":\"Louisiana\", \"ME\":\"Maine\", \"MD\":\"Maryland\", \"MA\":\"Massachusetts\", \"MI\":\"Michigan\", \"MN\":\"Minnesota\",\n",
        "          \"MS\":\"Mississippi\", \"MO\":\"Missouri\", \"MT\":\"Montana\", \"NE\":\"Nebraska\", \"NV\":\"Nevada\", \"NH\":\"New Hampshire\", \"NJ\":\"New Jersey\", \n",
        "          \"NM\":\"New Mexico\", \"NY\":\"New York\", \"NC\":\"North Carolina\", \"ND\":\"North Dakota\", \"OH\":\"Ohio\", \"OK\":\"Oklahoma\", \"OR\":\"Oregon\", \n",
        "          \"PA\":\"Pennsylvania\", \"RI\":\"Rhode Island\", \"SC\":\"South Carolina\", \"SD\":\"South Dakota\", \"TN\":\"Tennessee\", \"TX\":\"Texas\", \"UT\":\"Utah\",\n",
        "          \"VT\":\"Vermont\", \"VA\":\"Virginia\", \"WA\":\"Washington\", \"WV\":\"West Virginia\", \"WI\":\"Wisconsin\", \"WY\":\"Wyoming\", \"NYC\":\"New York\"}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtPdTSBfauwp"
      },
      "source": [
        "## NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55F8L6i8ax_j"
      },
      "source": [
        "### Splitting The Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yWvgMAkaxJQ"
      },
      "source": [
        "def cleanpunc(sentence):\n",
        "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
        "    cleaned = re.sub(r'[.|,|)|(|)|\\|/||:]',r' ',cleaned)      \n",
        "    return cleaned\n",
        "\n",
        "def acronym(s, dct):\n",
        "  return ' '.join([dct.get(i, i) for i in s.split()])\n",
        "\n",
        "for data in (train,test):\n",
        "    full_text_dict = {}\n",
        "    for i,row in data.iterrows():\n",
        "            full_text_dict[i] = acronym(cleanpunc(row['full_text']),states)\n",
        "            #print(i)       \n",
        "    data['full_txt2'] = data.index.map(full_text_dict)\n",
        "\n",
        "for data in (train,test):\n",
        "    data['num_word'] = data['tweet'].apply(lambda x : len(x.split()))\n",
        "    data['num_char'] = data['tweet'].apply(lambda x : len(x))\n",
        "    data['avg_word_length']= data['num_char']+1/data['num_word']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMWfVG4aa80-"
      },
      "source": [
        "### Handling Mentions and Links"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT-IqwOTa-6O"
      },
      "source": [
        "for data in (train,test):\n",
        "    linkword = \"{link}\"\n",
        "    link_dict = {}\n",
        "    for idx in range(len(data)):\n",
        "        text = data.loc[idx,'full_text']\n",
        "        if linkword in text.split():\n",
        "            link_dict[idx]=1\n",
        "        else:\n",
        "            link_dict[idx]=0\n",
        "    data['link_word'] = data.index.map(link_dict)\n",
        "\n",
        "for data in (train,test):\n",
        "    mention= \"@mention\"\n",
        "    mention_dict = {}\n",
        "    for idx in range(len(data)):\n",
        "        text = data.loc[idx,'full_text']\n",
        "        if mention in cleanpunc(text).split():\n",
        "            mention_dict[idx]=1\n",
        "        else:\n",
        "            mention_dict[idx]=0\n",
        "    data['mention_word'] = data.index.map(mention_dict)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtCvwfGObX7a"
      },
      "source": [
        "### Cleaning Punctuation and Links In Sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyEMnfyvbcKM",
        "outputId": "244db9e4-1d11-4887-fef2-e53eaa78b768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "ps = PorterStemmer()\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def cleanhtml (sentence):\n",
        "    cleantext = re.sub(r'http\\S+',r'',sentence)\n",
        "    return cleantext\n",
        "\n",
        "def cleanpunc(sentence):\n",
        "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
        "    cleaned = re.sub(r'[.|,|)|(|)|\\|/|:]',r' ',cleaned)      \n",
        "    return cleaned"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awQDqXe6bl20"
      },
      "source": [
        "for data in (train,test):\n",
        "    str1=' '\n",
        "    final_string=[]\n",
        "    s=''\n",
        "    for sent in data['full_text']:\n",
        "        filter_sent = []\n",
        "        rem_html = cleanhtml(sent)\n",
        "        rem_punc = cleanpunc (rem_html)\n",
        "        for w in rem_punc.split():\n",
        "            if ((w.isalpha())):\n",
        "                if (w.lower() not in stopwords):\n",
        "                    s=(ps.stem(w.lower())).encode('utf8')\n",
        "                    s=(w.lower()).encode('utf8')\n",
        "                    filter_sent.append(s)\n",
        "                else:\n",
        "                    continue\n",
        "            else:\n",
        "                continue\n",
        "        str1 = b\" \".join(filter_sent)\n",
        "        final_string.append(str1)\n",
        "    data['clean_text'] = np.array(final_string)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9jLHMMRbwKe"
      },
      "source": [
        "### Vectorizing and Truncating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74kvfjgNbmfb"
      },
      "source": [
        "vectorizer = TfidfVectorizer(encoding='utf-8', ngram_range=(1, 3), max_df=0.75, min_df=5) #max_features=12500\n",
        "\n",
        "train_vec = vectorizer.fit_transform(train.clean_text)\n",
        "test_vec = vectorizer.transform(test.clean_text)\n",
        "\n",
        "svd = TruncatedSVD(n_components=2, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)\n",
        "train_svd = svd.fit_transform(train_vec)\n",
        "test_svd = svd.transform(test_vec)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4oV3woxb1QP"
      },
      "source": [
        "col = ['svd1','svd2']\n",
        "svd_train =  pd.DataFrame(train_svd, columns=col)\n",
        "svd_test = pd.DataFrame(test_svd, columns=col)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c158tjpfb3I0",
        "outputId": "1a06e5bc-3572-403a-ae7c-110207b44a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ohe = OneHotEncoder()\n",
        "vectorizer2 = TfidfVectorizer(encoding='utf-8', ngram_range=(1, 3), max_df=0.75, min_df=5,max_features=50000)\n",
        "\n",
        "X = sp.sparse.hstack((vectorizer2.fit_transform(train.clean_text),normalize(train[['num_word', 'num_char', 'avg_word_length']].values),train[['missing_loc', 'missing_state','link_word', 'mention_word']],svd_train [['svd1','svd2']]),format='csr')\n",
        "\n",
        "X_columns=vectorizer.get_feature_names()+train[['num_word', 'num_char', 'avg_word_length','missing_loc', 'missing_state','link_word', 'mention_word']].columns.tolist() + svd_train [['svd1','svd2']].columns.tolist()\n",
        "print(X.shape)\n",
        "test_sp = sp.sparse.hstack((vectorizer2.transform(test.clean_text),normalize(test[['num_word', 'num_char', 'avg_word_length']].values),test[['missing_loc', 'missing_state','link_word', 'mention_word']],svd_test [['svd1','svd2']]),format='csr')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(77946, 32439)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6puAtvFcC2u",
        "outputId": "46ae579d-e0d8-485d-98d4-9f09f3a19116",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_columns=vectorizer.get_feature_names()+test[['num_word', 'num_char', 'avg_word_length','missing_loc', 'missing_state','link_word', 'mention_word']].columns.tolist() + svd_test [['svd1','svd2']].columns.tolist()\n",
        "\n",
        "print(test_sp.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(42157, 32439)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSiTU1GqdBj7"
      },
      "source": [
        "## Buidling a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgHW3dGNdaiA"
      },
      "source": [
        "### Splitting Data into train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eReBRCIcD4I",
        "outputId": "c49c57f2-2a66-4969-ee01-4848d226edc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "# Splitting Into train and test\n",
        "x, x_test, y, y_test = train_test_split(X,target,test_size=0.2,train_size=0.8, random_state = 0)\n",
        "\n",
        "cols = target.columns\n",
        "params = {'n_estimators': 500, 'max_depth': 3, 'min_samples_split': 2,\n",
        "          'learning_rate': 0.01, 'loss': 'ls'}\n",
        "dict_preds = {}\n",
        "\n",
        "clf = Ridge(random_state=25)\n",
        "for col in cols:\n",
        "    print('start',col)\n",
        "    clf.fit(x,y[col])\n",
        "    rmse = sqrt(mean_squared_error(y_test[col], clf.predict(x_test)))\n",
        "    print(\"RMSE: %.4f\" % rmse)\n",
        "    dict_preds[col]=clf.predict(test_sp)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start k1\n",
            "RMSE: 0.0691\n",
            "start k10\n",
            "RMSE: 0.1242\n",
            "start k11\n",
            "RMSE: 0.0856\n",
            "start k12\n",
            "RMSE: 0.1363\n",
            "start k13\n",
            "RMSE: 0.1511\n",
            "start k14\n",
            "RMSE: 0.0504\n",
            "start k15\n",
            "RMSE: 0.0842\n",
            "start k2\n",
            "RMSE: 0.1420\n",
            "start k3\n",
            "RMSE: 0.0471\n",
            "start k4\n",
            "RMSE: 0.1569\n",
            "start k5\n",
            "RMSE: 0.0697\n",
            "start k6\n",
            "RMSE: 0.0216\n",
            "start k7\n",
            "RMSE: 0.2419\n",
            "start k8\n",
            "RMSE: 0.0324\n",
            "start k9\n",
            "RMSE: 0.1461\n",
            "start s1\n",
            "RMSE: 0.1145\n",
            "start s2\n",
            "RMSE: 0.2436\n",
            "start s3\n",
            "RMSE: 0.2168\n",
            "start s4\n",
            "RMSE: 0.2297\n",
            "start s5\n",
            "RMSE: 0.2079\n",
            "start w1\n",
            "RMSE: 0.2833\n",
            "start w2\n",
            "RMSE: 0.1760\n",
            "start w3\n",
            "RMSE: 0.2315\n",
            "start w4\n",
            "RMSE: 0.1546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLUZ1pVQdWSw"
      },
      "source": [
        "### Checking Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyHCv-L1dMok",
        "outputId": "096439bc-e22e-4004-8286-0feb64e9be5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "regressor = RegressorChain(Ridge(random_state=25))\n",
        "regressor.fit(x,y)\n",
        "predictions = regressor.predict(x_test)\n",
        "sqrt(mean_squared_error(y_test,predictions))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1601981067951308"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}